# -*- coding: utf-8 -*-
"""resnet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hi9EjXpH8nfnwnkd3IsgfvCliYNhdwne
"""

import random
from typing import Tuple, List, Dict
import numpy as np
import pandas as pd
from PIL import Image
import cv2
import matplotlib.pyplot as plt
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, models
import torchvision.transforms.functional as TF
from sklearn.model_selection import train_test_split
import albumentations as A
from albumentations.pytorch import ToTensorV2

import os
import sys
import subprocess

# install packages if not already installed
subprocess.check_call([sys.executable, "-m", "pip", "install", "albumentations", "opencv-python"])

DATA_DIR = "/home/archit0929/skin"  # change USERNAME to your VM username
CSV_PATH = os.path.join(DATA_DIR, "HAM10000_metadata.csv")
IMAGES_DIRS = [
    os.path.join(DATA_DIR, "HAM10000_images_part_1"),
    os.path.join(DATA_DIR, "HAM10000_images_part_2")
]

import pandas as pd
df = pd.read_csv(CSV_PATH)
df.head()

MODEL_DIR = "saved_models"
os.makedirs(MODEL_DIR, exist_ok=True)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
SEED = 42
IMG_SIZE = 224        # classification image size
SEG_SIZE = 256        # segmentation input size
BATCH_SIZE = 8
SEG_EPOCHS = 12
CLS_EPOCHS = 12
LEARNING_RATE = 1e-4

random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
CLASS_MAP = {
    'akiec': 'Actinic keratoses',
    'bcc': 'Basal cell carcinoma',
    'bkl': 'Benign keratosis-like lesions',
    'df': 'Dermatofibroma',
    'mel': 'Melanoma',
    'nv': 'Melanocytic nevi',
    'vasc': 'Vascular lesions'
}
CLASSES = list(CLASS_MAP.keys())
NUM_CLASSES = len(CLASSES)
CLASS_TO_IDX = {k: i for i, k in enumerate(CLASSES)}
IDX_TO_CLASS = {v: k for k, v in CLASS_TO_IDX.items()}

def read_image(path, color=True):
    img = cv2.imread(path)
    if img is None:
        raise FileNotFoundError(path)
    if color:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    else:
        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    return img

def ensure_rgb(img):
    # PIL -> numpy or cv2 ensures shape HWC
    if len(img.shape) == 2:
        img = np.stack([img]*3, axis=-1)
    if img.shape[2] == 4:
        img = img[..., :3]
    return img

import glob
# Removed find_image_path as we will pre-build a mapping

class HAMSegmentationDataset(Dataset):
    """
    For segmentation: returns image and mask.
    If masks are not present, creates rough masks using Otsu thresholding as fallback.
    Requires metadata CSV with image_id or lesion_id and filename.
    """
    def __init__(self, df, image_path_map, transforms=None, target_size=SEG_SIZE):
        self.df = df.reset_index(drop=True)
        self.image_path_map = image_path_map # Use the pre-built map
        self.transforms = transforms
        self.target_size = target_size

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image_id = row['image_id'] if 'image_id' in row else row['image_id']
        img_path = self.image_path_map.get(image_id + ".jpg") # Get path from map

        if img_path is None:
            print(f"Warning: Image {image_id}.jpg not found in map. Skipping.")
            # fallback: sample another random item to avoid crashing the dataloader
            new_idx = random.randint(0, len(self.df) - 1)
            return self.__getitem__(new_idx)

        img = read_image(img_path)
        img = ensure_rgb(img)
        # segmentation mask: if user has supplied masks, load them.
        # HAM10000 does not include masks by default; we will make a weak mask via simple preprocessing.
        mask = create_weak_mask(img)

        # Ensure image and mask are resized to target_size before transformations
        img = cv2.resize(img, (self.target_size, self.target_size))
        mask = cv2.resize(mask, (self.target_size, self.target_size), interpolation=cv2.INTER_NEAREST)


        # Albumentations expects HWC numpy arrays
        if self.transforms:
            augmented = self.transforms(image=img, mask=mask)
            image = augmented['image']
            mask = augmented['mask'].unsqueeze(0).float()
        else:
            image = TF.to_tensor(Image.fromarray(img)) # Removed resize here
            mask = TF.to_tensor(Image.fromarray(mask)).float() # Removed resize here

        return image, mask

def create_weak_mask(img: np.ndarray) -> np.ndarray:
    """
    Create a simple mask for lesions using color space heuristics + Otsu.
    This is not perfect but often good enough to train the segmentation model if ground truths are not available.
    """
    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    blur = cv2.GaussianBlur(gray, (5,5), 0)
    # Otsu threshold
    _, otsu = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
    # morphological ops
    kernel = np.ones((5,5), np.uint8)
    opening = cv2.morphologyEx(otsu, cv2.MORPH_OPEN, kernel, iterations=1)
    # fill holes
    contours, _ = cv2.findContours(opening, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    mask = np.zeros_like(opening)
    if contours:
        largest = max(contours, key=cv2.contourArea)
        cv2.drawContours(mask, [largest], -1, 255, -1)
    mask = mask // 255
    return mask.astype(np.uint8)

class HAMClassificationDataset(Dataset):
    """
    For classification: uses H x W images (cropped using segmentation mask optionally)
    Expects 'image_id' and 'dx' columns in dataframe (dx = lesion label like 'nv', 'mel', etc.)
    """
    def __init__(self, df, image_path_map, transforms=None, seg_model=None, seg_size=SEG_SIZE, cls_size=IMG_SIZE, use_segmentation=True):
        self.df = df.reset_index(drop=True)
        self.image_path_map = image_path_map # Use the pre-built map
        self.transforms = transforms
        self.seg_model = seg_model
        self.seg_size = seg_size
        self.cls_size = cls_size
        self.use_segmentation = use_segmentation

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image_id = row['image_id']
        label = row['dx']
        img_path = self.image_path_map.get(image_id + ".jpg") # Get path from map

        if img_path is None:
            # fallback: sample another random item to avoid crashing the dataloader
            new_idx = random.randint(0, len(self.df) - 1)
            print(f"Warning: Image {image_id}.jpg not found in map. Skipping.")
            return self.__getitem__(new_idx)

        img = read_image(img_path)
        img = ensure_rgb(img)

        # optional segmentation-guided crop
        if self.use_segmentation and self.seg_model is not None:
            # prepare seg input (resize -> normalize -> to tensor)
            seg_input = cv2.resize(img, (self.seg_size, self.seg_size))
            seg_input = seg_input.astype(np.float32) / 255.0
            seg_input = np.transpose(seg_input, (2, 0, 1))
            seg_input = torch.tensor(seg_input).unsqueeze(0).to(DEVICE)

            with torch.no_grad():
                pred_mask = self.seg_model(seg_input)[0]             # logits
                pred_mask = torch.sigmoid(pred_mask).cpu().numpy().squeeze()
                bmask = (pred_mask > 0.5).astype(np.uint8)
                # resize mask back to original image size
                bmask = cv2.resize(bmask, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)

            x, y, w, h = mask_to_bbox(bmask)
            if w > 0 and h > 0:
                pad = int(0.15 * max(w, h))
                x1 = max(0, x - pad); y1 = max(0, y - pad)
                x2 = min(img.shape[1], x + w + pad); y2 = min(img.shape[0], y + h + pad)
                crop = img[y1:y2, x1:x2]
            else:
                crop = img
        else:
            crop = img

        # Ensure we always resize the crop to a fixed classification size
        crop = cv2.resize(crop, (self.cls_size, self.cls_size), interpolation=cv2.INTER_LINEAR)

        if self.transforms:
            augmented = self.transforms(image=crop)
            image = augmented['image']
        else:
            image = TF.to_tensor(Image.fromarray(crop))

        return image, CLASS_TO_IDX[label]

def mask_to_bbox(mask: np.ndarray) -> Tuple[int,int,int,int]:
    ys, xs = np.where(mask>0)
    if len(xs)==0 or len(ys)==0:
        return 0,0,0,0
    x1, x2 = xs.min(), xs.max()
    y1, y2 = ys.min(), ys.max()
    return x1, y1, x2-x1, y2-y1

class ResNet50Encoder(nn.Module):
    def __init__(self, pretrained=True):
        super().__init__()
        backbone = models.resnet50(pretrained=pretrained)
        # extract blocks
        self.initial = nn.Sequential(
            backbone.conv1,
            backbone.bn1,
            backbone.relu,
            backbone.maxpool
        )  # out /4
        self.layer1 = backbone.layer1  # out /4
        self.layer2 = backbone.layer2  # out /8
        self.layer3 = backbone.layer3  # out /16
        self.layer4 = backbone.layer4  # out /32

    def forward(self, x):
        x0 = self.initial(x)
        x1 = self.layer1(x0)
        x2 = self.layer2(x1)
        x3 = self.layer3(x2)
        x4 = self.layer4(x3)
        return x0, x1, x2, x3, x4

class DecoderBlock(nn.Module):
    def __init__(self, in_ch, out_ch):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_ch, out_ch, kernel_size=3, padding=1),
            nn.BatchNorm2d(out_ch),
            nn.ReLU(inplace=True),
        )

    def forward(self, x, skip=None): # Added skip=None for consistency
        if skip is not None:
            # Use explicit size (H, W) from skip for upsampling
            target_h, target_w = skip.size(2), skip.size(3)
            x = F.interpolate(x, size=(target_h, target_w), mode='bilinear', align_corners=False)
            # safety check (useful while debugging)
            assert x.size(2) == skip.size(2) and x.size(3) == skip.size(3), \
                f"Spatial mismatch after upsample: x={x.size()}, skip={skip.size()}"
            x = torch.cat([x, skip], dim=1)
        else:
            # If no skip, assume simple upsampling by 2
            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)
        x = self.conv(x)
        return x

class ResNetUNet(nn.Module):
    def __init__(self, n_classes=1, pretrained=True):
        super().__init__()
        self.encoder = ResNet50Encoder(pretrained=pretrained)
        # the channel dims from ResNet50
        chs = [64, 256, 512, 1024, 2048]
        # Decoder blocks with correct input and output channels
        # The input to decoder4 is x4 (2048) and skip x3 (1024) -> 2048 + 1024 = 3072
        self.decoder4 = DecoderBlock(chs[4] + chs[3], 512) # input 3072, output 512, upsamples x4 to match x3
        # input to decoder3 is output of decoder4 (512) and skip x2 (512) -> 512 + 512 = 1024
        self.decoder3 = DecoderBlock(512 + chs[2], 256) # input 1024, output 256, upsamples decoder4 output to match x2
        # input to decoder2 is output of decoder3 (256) and skip x1 (256) -> 256 + 256 = 512
        self.decoder2 = DecoderBlock(256 + chs[1], 128) # input 512, output 128, upsamples decoder3 output to match x1
        # input to decoder1 is output of decoder2 (128) and skip x0 (64) -> 128 + 64 = 192
        self.decoder1 = DecoderBlock(128 + chs[0], 64) # input 192, output 64, upsamples decoder2 output to match x0
        # The initial block output x0 is 1/4 of the original size.
        # To get back to the original size, we need two more upsampling steps (1/4 -> 1/2 -> 1)
        # input to decoder0 is output of decoder1 (64), no skip -> 64
        self.decoder0 = DecoderBlock(64, 32) # input 64, output 32, upsamples decoder1 output by 2 (1/4 -> 1/2)
        # input to decoder_out is output of decoder0 (32), no skip -> 32
        self.decoder_out = DecoderBlock(32, 16) # input 32, output 16, upsamples decoder0 output by 2 (1/2 -> 1)

        self.final_conv = nn.Conv2d(16, n_classes, kernel_size=1) # input 16, output n_classes

    def forward(self, x):
        x0, x1, x2, x3, x4 = self.encoder(x)
        d4 = self.decoder4(x4, x3)
        d3 = self.decoder3(d4, x2)
        d2 = self.decoder2(d3, x1)
        d1 = self.decoder1(d2, x0)
        d0 = self.decoder0(d1) # No skip connection here
        d_out = self.decoder_out(d0) # No skip connection here
        out = self.final_conv(d_out)
        return out  # logits

class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.shared_MLP = nn.Sequential(
            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),
            nn.ReLU(),
            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)
        )
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg = self.shared_MLP(self.avg_pool(x))
        max_ = self.shared_MLP(self.max_pool(x))
        out = avg + max_
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        assert kernel_size in (3,7)
        padding = 3 if kernel_size==7 else 1
        self.conv = nn.Conv2d(2,1,kernel_size,padding=padding,bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg = torch.mean(x, dim=1, keepdim=True)
        max_, _ = torch.max(x, dim=1, keepdim=True)
        cat = torch.cat([avg, max_], dim=1)
        return self.sigmoid(self.conv(cat))

class CBAM(nn.Module):
    def __init__(self, in_planes, ratio=16, kernel_size=7):
        super().__init__()
        self.ca = ChannelAttention(in_planes, ratio)
        self.sa = SpatialAttention(kernel_size)

    def forward(self, x):
        out = x * self.ca(x)
        out = out * self.sa(out)
        return out

# 3) DenseNet121 with CBAM head
class DenseNetCBAMClassifier(nn.Module):
    def __init__(self, num_classes=NUM_CLASSES, pretrained=True):
        super().__init__()
        backbone = models.densenet121(pretrained=pretrained)
        # remove classifier
        self.features = backbone.features  # feature extractor
        num_features = backbone.classifier.in_features
        self.cbam = CBAM(num_features)
        self.pool = nn.AdaptiveAvgPool2d((1,1))
        self.classifier = nn.Linear(num_features, num_classes)

    def forward(self, x):
        features = self.features(x)  # (B, C, H, W)
        # apply cbam
        features = self.cbam(features)
        out = self.pool(features).view(features.size(0), -1)
        logits = self.classifier(out)
        return logits

# ---------------------------
# Losses & metrics
# ---------------------------
def dice_loss(pred, target, eps=1e-6):
    pred = torch.sigmoid(pred)
    num = 2 * (pred * target).sum(dim=(2,3))
    den = pred.sum(dim=(2,3)) + target.sum(dim=(2,3)) + eps
    loss = 1 - (num / den)
    return loss.mean()

def train_segmentation(seg_model, train_loader, val_loader, epochs=SEG_EPOCHS):
    seg_model = seg_model.to(DEVICE)
    opt = torch.optim.Adam(seg_model.parameters(), lr=LEARNING_RATE)
    best_val = 1e9
    for epoch in range(epochs):
        seg_model.train()
        sum_loss = 0.0
        for imgs, masks in train_loader:
            imgs = imgs.to(DEVICE)
            masks = masks.to(DEVICE)
            logits = seg_model(imgs)
            loss_bce = F.binary_cross_entropy_with_logits(logits, masks)
            loss_dice = dice_loss(logits, masks)
            loss = loss_bce + loss_dice
            opt.zero_grad(); loss.backward(); opt.step()
            sum_loss += loss.item() * imgs.size(0)
        train_loss = sum_loss / len(train_loader.dataset)

        # validation
        seg_model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for imgs, masks in val_loader:
                imgs = imgs.to(DEVICE); masks = masks.to(DEVICE)
                logits = seg_model(imgs)
                loss = F.binary_cross_entropy_with_logits(logits, masks) + dice_loss(logits, masks)
                val_loss += loss.item() * imgs.size(0)
        val_loss = val_loss / len(val_loader.dataset)
        print(f"[Seg] Epoch {epoch+1}/{epochs} — train_loss: {train_loss:.4f}, val_loss: {val_loss:.4f}")
        if val_loss < best_val:
            best_val = val_loss
            torch.save(seg_model.state_dict(), os.path.join(MODEL_DIR, "best_seg.pth"))
            print("Saved best_seg.pth")
    # load best
    seg_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, "best_seg.pth")))
    seg_model.eval()
    return seg_model

def train_classification(cls_model, train_loader, val_loader, epochs=CLS_EPOCHS):
    cls_model = cls_model.to(DEVICE)
    opt = torch.optim.Adam(cls_model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='max', factor=0.5, patience=3)
    best_acc = 0.0
    criterion = nn.CrossEntropyLoss()
    for epoch in range(epochs):
        cls_model.train()
        running = 0.0
        correct = 0
        total = 0
        for imgs, labels in train_loader:
            imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)
            logits = cls_model(imgs)
            loss = criterion(logits, labels)
            opt.zero_grad(); loss.backward(); opt.step()
            running += loss.item() * imgs.size(0)
            preds = torch.argmax(logits, dim=1)
            correct += (preds == labels).sum().item()
            total += imgs.size(0)
        train_loss = running / total
        train_acc = correct / total

        # validation
        cls_model.eval()
        v_running = 0.0
        v_correct = 0
        v_total = 0
        with torch.no_grad():
            for imgs, labels in val_loader:
                imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)
                logits = cls_model(imgs)
                loss = criterion(logits, labels)
                v_running += loss.item() * imgs.size(0)
                preds = torch.argmax(logits, dim=1)
                v_correct += (preds == labels).sum().item()
                v_total += imgs.size(0)
        val_loss = v_running / v_total
        val_acc = v_correct / v_total
        scheduler.step(val_acc)
        print(f"[Cls] Epoch {epoch+1}/{epochs} — train_loss: {train_loss:.4f}, train_acc: {train_acc:.4f}, val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}")
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(cls_model.state_dict(), os.path.join(MODEL_DIR, "best_cls.pth"))
            print("Saved best_cls.pth")
    cls_model.load_state_dict(torch.load(os.path.join(MODEL_DIR, "best_cls.pth")))
    cls_model.eval()
    return cls_model

class GradCAM:
    def __init__(self, model: nn.Module, target_layer):
        self.model = model
        self.model.eval()
        self.gradients = None
        self.activations = None
        # hook to save activations and gradients
        def forward_hook(module, inp, out):
            self.activations = out.detach()
        def backward_hook(module, grad_in, grad_out):
            self.gradients = grad_out[0].detach()
        target_layer.register_forward_hook(forward_hook)
        target_layer.register_backward_hook(backward_hook)

    def __call__(self, input_tensor: torch.Tensor, class_idx=None):
        """
        input_tensor: 1xCxHxW (on DEVICE)
        returns heatmap (H x W numpy), pred_class_index, probs
        """
        logits = self.model(input_tensor)
        probs = F.softmax(logits, dim=1).cpu().detach().numpy()[0]
        pred_class = int(torch.argmax(logits, dim=1).cpu().numpy()[0]) if class_idx is None else class_idx
        score = logits[0, pred_class]
        self.model.zero_grad()
        score.backward(retain_graph=True)
        grads = self.gradients  # (C, H, W)
        acts = self.activations  # (C, H, W)
        weights = grads.mean(dim=(1,2), keepdim=True)  # global average pool over H,W -> (C,1,1)
        cam = (weights * acts).sum(dim=0, keepdim=False)  # (H,W)
        cam = F.relu(cam)
        cam_np = cam.cpu().numpy()
        cam_np = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min() + 1e-8)
        # resize to input size
        cam_resized = cv2.resize(cam_np, (input_tensor.size(3), input_tensor.size(2)))
        return cam_resized, pred_class, probs

def prepare_data(csv_path=CSV_PATH, images_dir=IMAGES_DIRS, test_size=0.2, val_size=0.1):
    df = pd.read_csv(csv_path)
    # ensure image_id column exists; common CSV uses 'image_id' or 'image_id'
    if 'image_id' not in df.columns and 'image_id' in df.columns:
        df.rename(columns={'image_id': 'image_id'}, inplace=True)
    # Keep only rows with known dx in our CLASS_MAP
    df = df[df['dx'].isin(CLASSES)].reset_index(drop=True)
    # stratified split
    train_val, test = train_test_split(df, test_size=test_size, stratify=df['dx'], random_state=SEED)
    train, val = train_test_split(train_val, test_size=val_size/(1-test_size), stratify=train_val['dx'], random_state=SEED)
    print(f"Samples -> train: {len(train)}, val: {len(val)}, test: {len(test)}")
    return train.reset_index(drop=True), val.reset_index(drop=True), test.reset_index(drop=True)

def evaluate_classification(model, loader):
    model.eval()
    total = 0
    correct = 0
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for imgs, labels in loader:
            imgs = imgs.to(DEVICE); labels = labels.to(DEVICE)
            logits = model(imgs)
            preds = torch.argmax(logits, dim=1)
            correct += (preds == labels).sum().item()
            total += imgs.size(0)
            all_preds.extend(preds.cpu().numpy().tolist())
            all_labels.extend(labels.cpu().numpy().tolist())
    acc = correct / total
    print(f"Test classification accuracy: {acc:.4f}")

def run_gradcam_example(model, dataset, num_examples=3):
    # pick a few examples
    for i in range(num_examples):
        img, label = dataset[i]
        input_tensor = img.unsqueeze(0).to(DEVICE)
        # pick a target layer in the classifier features
        target_layer = model.features[-1]  # last block of DenseNet features
        gradcam = GradCAM(model, target_layer)
        cam, pred_class, probs = gradcam(input_tensor)
        # prepare visualization
        orig = img.permute(1,2,0).cpu().numpy()
        mean = np.array([0.485,0.456,0.406])
        std = np.array([0.229,0.224,0.225])
        orig = (orig * std + mean)
        orig = np.clip(orig, 0, 1)
        heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB) / 255.0
        blended = 0.5 * orig + 0.5 * heatmap
        plt.figure(figsize=(8,4))
        plt.subplot(1,3,1); plt.imshow(orig); plt.title(f"GT: {CLASS_MAP[CLASSES[label]]}"); plt.axis('off')
        plt.subplot(1,3,2); plt.imshow(cam, cmap='jet'); plt.title("Grad-CAM"); plt.axis('off')
        plt.subplot(1,3,3); plt.imshow(blended); plt.title(f"Pred: {CLASS_MAP[CLASSES[pred_class]]}\nProbs: {np.round(probs,3)}"); plt.axis('off')
        plt.show()

import pandas as pd

image_dirs = [
    '/home/archit0929/skin/HAM10000_images_part_1',
    '/home/archit0929/skin/HAM10000_images_part_2'
]


all_images = set()
for d in image_dirs:
    all_images.update(os.listdir(d))

# assuming your dataframe or csv column has 'image_id' column
df = pd.read_csv('/home/archit0929/skin/HAM10000_metadata.csv')

missing = [img_id for img_id in df['image_id'] if f"{img_id}.jpg" not in all_images]

print(f"Missing {len(missing)} images:")
print(missing[:20])  # show first 20

# Build image path map
image_path_map = {}
for d in IMAGES_DIRS:
  for root, _, filenames in os.walk(d):
    for filename in filenames:
      if filename.endswith(".jpg") or filename.endswith(".jpeg"):
        image_path_map[filename] = os.path.join(root, filename)

# Prepare dataframes
train_df, val_df, test_df = prepare_data()

# Augmentations (segmentation)
seg_train_transforms = A.Compose([
    A.Resize(SEG_SIZE, SEG_SIZE),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.RandomRotate90(p=0.5),
    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
    ToTensorV2()
])
seg_val_transforms = A.Compose([
    A.Resize(SEG_SIZE, SEG_SIZE),
    A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
    ToTensorV2()
])

# Prepare segmentation datasets/loaders
seg_train_ds = HAMSegmentationDataset(train_df, image_path_map, transforms=seg_train_transforms, target_size=SEG_SIZE)
seg_val_ds = HAMSegmentationDataset(val_df, image_path_map, transforms=seg_val_transforms, target_size=SEG_SIZE)
seg_train_loader = DataLoader(seg_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
seg_val_loader = DataLoader(seg_val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)

# instantiate segmentation model
seg_model = ResNetUNet(n_classes=1, pretrained=True)
print("Training segmentation model...")
seg_model = train_segmentation(seg_model, seg_train_loader, seg_val_loader, epochs=SEG_EPOCHS)

def main():
    # Set multiprocessing start method to 'spawn' for compatibility with CUDA
    import torch.multiprocessing as mp
    try:
        mp.set_start_method('spawn', force=True)
    except RuntimeError:
        pass # 'spawn' is already set or not supported

    preprocessed_dir = os.path.join(DATA_DIR, "preprocessed_images")
    os.makedirs(preprocessed_dir, exist_ok=True)
    preprocess_flag_file = os.path.join(preprocessed_dir, "preprocessing_complete.flag")

    # Check if preprocessing is already complete
    if not os.path.exists(preprocess_flag_file):
        # Pre-process images using the segmentation model and save them
        def preprocess_and_save(df, dataset_type):
            ds = HAMClassificationDataset(df, image_path_map, transforms=None, seg_model=seg_model, use_segmentation=True, cls_size=IMG_SIZE)
            print(f"Preprocessing {dataset_type} dataset...")
            for i in range(len(ds)):
                image_id = df.iloc[i]['image_id']
                save_path_img = os.path.join(preprocessed_dir, f"{image_id}_{dataset_type}.png")
                save_path_label = os.path.join(preprocessed_dir, f"{image_id}_{dataset_type}.txt")

                try:
                    img_tensor, label = ds[i]
                    # Convert tensor to numpy array (HWC)
                    img_np = img_tensor.permute(1, 2, 0).cpu().numpy()
                    # Convert float [0,1] to uint8 [0,255]
                    img_np = (img_np * 255).astype(np.uint8)
                    # Save image
                    cv2.imwrite(save_path_img, cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR))
                    # Save label
                    with open(save_path_label, "w") as f:
                        f.write(str(label))
                except Exception as e:
                    print(f"Error processing image {df.iloc[i]['image_id']}: {e}")


        # Only preprocess if the flag file does not exist
        preprocess_and_save(train_df, "train")
        preprocess_and_save(val_df, "val")
        preprocess_and_save(test_df, "test")

        # Create the flag file to indicate preprocessing is complete
        with open(preprocess_flag_file, "w") as f:
            f.write("Preprocessing complete.")

    # Load pre-processed data into memory
    print("Loading pre-processed data into memory...")
    def load_preprocessed_data(df, preprocessed_dir, dataset_type):
        images = []
        labels = []
        for i in range(len(df)):
            image_id = df.iloc[i]['image_id']
            img_path = os.path.join(preprocessed_dir, f"{image_id}_{dataset_type}.png")
            label_path = os.path.join(preprocessed_dir, f"{image_id}_{dataset_type}.txt")
            if os.path.exists(img_path) and os.path.exists(label_path):
                img = read_image(img_path)
                with open(label_path, "r") as f:
                    label = int(f.read())
                images.append(img)
                labels.append(label)
        return images, labels

    train_images, train_labels = load_preprocessed_data(train_df, preprocessed_dir, "train")
    val_images, val_labels = load_preprocessed_data(val_df, preprocessed_dir, "val")
    test_images, test_labels = load_preprocessed_data(test_df, preprocessed_dir, "test")


    # Define a new dataset class to load data from memory
    class InMemClassificationDataset(Dataset):
        def __init__(self, images, labels, transforms=None):
            self.images = images
            self.labels = labels
            self.transforms = transforms

        def __len__(self):
            return len(self.images)

        def __getitem__(self, idx):
            img = self.images[idx]
            label = self.labels[idx]

            if self.transforms:
                augmented = self.transforms(image=img)
                image = augmented['image']
            else:
                 # Ensure image is in (C, H, W) format and is a float tensor if no transforms
                image = TF.to_tensor(Image.fromarray(img))


            return image, label

    # Augmentations (classification)
    cls_transforms = A.Compose([
        A.Resize(IMG_SIZE, IMG_SIZE),
        A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.1, rotate_limit=20, p=0.5),
        A.RandomBrightnessContrast(p=0.5),
        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),
        ToTensorV2()
    ])

    # Prepare classification datasets/loaders using in-memory data
    cls_train_ds = InMemClassificationDataset(train_images, train_labels, transforms=cls_transforms)
    cls_val_ds = InMemClassificationDataset(val_images, val_labels, transforms=cls_transforms)
    cls_test_ds = InMemClassificationDataset(test_images, test_labels, transforms=cls_transforms)


    cls_train_loader = DataLoader(cls_train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)
    cls_val_loader = DataLoader(cls_val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)
    cls_test_loader = DataLoader(cls_test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)


    cls_model = DenseNetCBAMClassifier(num_classes=NUM_CLASSES, pretrained=True)

    print("Training classification model...")
    cls_model = train_classification(cls_model, cls_train_loader, cls_val_loader, epochs=CLS_EPOCHS)
    evaluate_classification(cls_model, cls_test_loader)
    run_gradcam_example(cls_model, cls_test_ds, num_examples=3)

if __name__ == "__main__":
    main()